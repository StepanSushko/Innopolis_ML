{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efca6a7e",
   "metadata": {},
   "source": [
    "# Домашнее задание 22: TF-IDF и Word2Vec на корпусе Helsinki-NLP/opus-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278acee4",
   "metadata": {},
   "source": [
    "## Установка зависимостей (при необходимости)\n",
    "При первом запуске раскомментируйте команды ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ed6bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets nltk pymorphy3 natasha gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c94bbe",
   "metadata": {},
   "source": [
    "## Импорт библиотек и подготовка инструментов\n",
    "Подключаем всё необходимое для обработки текста и моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02db05fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pymorphy3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "russian_stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa9811",
   "metadata": {},
   "source": [
    "## Загрузка корпуса и разбиение на обучающую и тестовую части\n",
    "Используем 1200 предложений: 1000 для обучения и 200 для проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "192371ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры исходных сплитов: train=1000000, val=2000, test=2000\n"
     ]
    }
   ],
   "source": [
    "pair = \"en-ru\"\n",
    "LANG_SRC, LANG_TGT = pair.split(\"-\")\n",
    "\n",
    "DATASET_REPO = \"Helsinki-NLP/opus-100\"\n",
    "\n",
    "def load_split(split: str):\n",
    "    filename = f\"{LANG_SRC}-{LANG_TGT}/{split}-00000-of-00001.parquet\"\n",
    "    local_path = hf_hub_download(DATASET_REPO, filename=filename, repo_type=\"dataset\")\n",
    "    df = pd.read_parquet(local_path)\n",
    "    return [{\"translation\": rec} for rec in df[\"translation\"].tolist()]\n",
    "\n",
    "train_ds = load_split(\"train\")\n",
    "val_ds   = load_split(\"validation\")\n",
    "test_ds  = load_split(\"test\")\n",
    "\n",
    "print(f\"Размеры исходных сплитов: train={len(train_ds)}, val={len(val_ds)}, test={len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f710f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ru",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "en",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "13095008-cb60-49fc-a9ed-5ac413f7a6f3",
       "rows": [
        [
         "0",
         "Да, но не совсем...",
         "Yeah, that's not exactly..."
        ],
        [
         "1",
         "!",
         "!"
        ],
        [
         "2",
         "Приводимое ниже расписание является предварительным; с самой последней информацией можно ознакомиться в Интернете по адресу www.un.org/News/ossg/conf.htm.",
         "The schedule below is tentative; up-to-date information can be obtained at www.un.org/News/ossg/conf.htm."
        ],
        [
         "3",
         "Но сейчас ...я вверяю вам удостовериться, что шотландцы приуменьшат",
         "But for now,"
        ],
        [
         "4",
         "Они тусовались там несколько недель.",
         "He'd been out there a few weeks or so."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ru</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Да, но не совсем...</td>\n",
       "      <td>Yeah, that's not exactly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Приводимое ниже расписание является предварите...</td>\n",
       "      <td>The schedule below is tentative; up-to-date in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Но сейчас ...я вверяю вам удостовериться, что ...</td>\n",
       "      <td>But for now,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Они тусовались там несколько недель.</td>\n",
       "      <td>He'd been out there a few weeks or so.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ru  \\\n",
       "0                                Да, но не совсем...   \n",
       "1                                                  !   \n",
       "2  Приводимое ниже расписание является предварите...   \n",
       "3  Но сейчас ...я вверяю вам удостовериться, что ...   \n",
       "4               Они тусовались там несколько недель.   \n",
       "\n",
       "                                                  en  \n",
       "0                        Yeah, that's not exactly...  \n",
       "1                                                  !  \n",
       "2  The schedule below is tentative; up-to-date in...  \n",
       "3                                       But for now,  \n",
       "4             He'd been out there a few weeks or so.  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = 200000  # ограничим объём для отладки и анализа\n",
    "train_subset = train_ds[:sample_size]\n",
    "train_df = pd.DataFrame({\n",
    "    'ru': [row['translation'][LANG_TGT] for row in train_subset],\n",
    "    'en': [row['translation'][LANG_SRC] for row in train_subset],\n",
    "})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8d4590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = test_ds[:200]\n",
    "test_df = pd.DataFrame({\n",
    "    'ru': [row['translation'][LANG_TGT] for row in test_subset],\n",
    "    'en': [row['translation'][LANG_SRC] for row in test_subset],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e044d5",
   "metadata": {},
   "source": [
    "## Определяем функции предварительной обработки\n",
    "Токенизация, приведение к нижнему регистру, лемматизация и очистка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee2b9481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ru",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "191aba63-1cb4-4d13-a90e-93945a029e81",
       "rows": [
        [
         "0",
         "[]"
        ],
        [
         "1",
         "[]"
        ],
        [
         "2",
         "['приводить', 'ниже', 'расписание', 'являться', 'предварительный', 'последний', 'информация', 'ознакомиться', 'интернет', 'адрес']"
        ],
        [
         "3",
         "['вверять', 'удостовериться', 'шотландец', 'приуменьшить']"
        ],
        [
         "4",
         "['тусоваться', 'несколько', 'неделя']"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "0                                                   []\n",
       "1                                                   []\n",
       "2    [приводить, ниже, расписание, являться, предва...\n",
       "3    [вверять, удостовериться, шотландец, приуменьш...\n",
       "4                      [тусоваться, несколько, неделя]\n",
       "Name: ru, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lowered = [token.lower() for token in tokens]\n",
    "    lemmas = []\n",
    "    for token in lowered:\n",
    "        if re.search('[а-яА-ЯёЁ]', token):\n",
    "            lemma = morph.parse(token)[0].normal_form\n",
    "            lemmas.append(lemma)\n",
    "    cleaned = [token for token in lemmas if re.fullmatch(r'[а-яё-]+', token) and token not in russian_stopwords]\n",
    "    return cleaned\n",
    "\n",
    "train_tokens = train_df['ru'].apply(preprocess_sentence)\n",
    "train_tokens.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ae6cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = test_df['ru'].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a95ee",
   "metadata": {},
   "source": [
    "## TF-IDF векторизация\n",
    "Обучаем TF-IDF на обучающих предложениях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2265447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 57540)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus = train_tokens.apply(lambda tokens: ' '.join(tokens))\n",
    "test_corpus = test_tokens.apply(lambda tokens: ' '.join(tokens))\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix_train = tfidf.fit_transform(train_corpus)\n",
    "tfidf_matrix_test = tfidf.transform(test_corpus)\n",
    "tfidf_matrix_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08037538",
   "metadata": {},
   "source": [
    "## Поиск ближайших предложений (TF-IDF)\n",
    "Находим топ-3 похожих обучающих предложений для нескольких тестовых примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "808d8a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестовое предложение: Только бы не вылететь.\n",
      "Топ-3 похожих предложения из тренирочного корпуса:\n",
      "  score=0.745 | Виктор так и не вылетел.\n",
      "  score=0.695 | - Я мог бы вылететь сегодня вечером.\n",
      "  score=0.676 | Шпилька вылетела\n",
      "\n",
      "Тестовое предложение: И как ты только справляешься, папа, таская эти коробки взад-вперед целый день.\n",
      "Топ-3 похожих предложения из тренирочного корпуса:\n",
      "  score=0.413 | их целая коробка стоит!\n",
      "  score=0.373 | Целый день?\n",
      "  score=0.368 | Ты справляешься?\n",
      "\n",
      "Тестовое предложение: Возможно, у нас есть небольшое преимущество в переговорах.\n",
      "Топ-3 похожих предложения из тренирочного корпуса:\n",
      "  score=0.509 | Небольшая.\n",
      "  score=0.509 | Да, да, после небольшого...\n",
      "  score=0.509 | Она совсем небольшая.\n",
      "\n",
      "Тестовое предложение: Сколько времени вы будете делать то, что ему нужно?\n",
      "Топ-3 похожих предложения из тренирочного корпуса:\n",
      "  score=0.729 | - Сколько у нас времени?\n",
      "  score=0.729 | Сколько времени?\n",
      "  score=0.729 | Сколько у нас есть времени?\n",
      "\n",
      "Тестовое предложение: 1 апреля Президент НКР Бако Саакян принял начальника Генштаба Вооруженных сил Республики Армения генерал-полковника Юрия Хачатурова.\n",
      "Топ-3 похожих предложения из тренирочного корпуса:\n",
      "  score=0.388 | Президент Бако Саакян подписал указ о внесении изменений и дополнений в устав и структуру Аппарата Президента Нагорно-Карабахской Республики, согласно которому должность начальника Главного информационного управления Аппарата Президента НКР переименована в начальника Главного информационного управления Аппарата Президента НКР - заместителя руководителя Аппарата Президента НКР.\n",
      "  score=0.324 | Согласно Бако Саакяну, можно думать, что наша республика получит международное признание после урегулирования конфликта.\n",
      "  score=0.320 | Бако.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def top_k_similar(vector, matrix, k=3):\n",
    "    similarities = cosine_similarity(vector, matrix)[0]\n",
    "    top_idx = similarities.argsort()[::-1][:k]\n",
    "    return list(zip(top_idx, similarities[top_idx]))\n",
    "\n",
    "for i in range(5):\n",
    "    vector = tfidf_matrix_test[i]\n",
    "    matches = top_k_similar(vector, tfidf_matrix_train)\n",
    "    sentence = test_df.loc[i, 'ru']\n",
    "    print(f'Тестовое предложение: {sentence}')\n",
    "    print('Топ-3 похожих предложения из тренирочного корпуса:')\n",
    "    for idx, score in matches:\n",
    "        ref_sentence = train_df.loc[idx, 'ru']\n",
    "        print(f'  score={score:.3f} | {ref_sentence}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e29a53",
   "metadata": {},
   "source": [
    "## Обучение модели Word2Vec\n",
    "Строим векторные представления слов на основе обучающего корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a98a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59645"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model = Word2Vec(sentences=train_tokens.tolist(), vector_size=300, window=5, min_count=1, workers=10, seed=42)\n",
    "w2v_model.wv.key_to_index.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821b811f",
   "metadata": {},
   "source": [
    "## Средние вектора предложений (Word2Vec)\n",
    "Представляем каждое предложение как средний вектор его слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45925328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_vector(tokens, model):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    return np.zeros(model.vector_size)\n",
    "\n",
    "train_vectors = np.vstack([sentence_vector(tokens, w2v_model) for tokens in train_tokens])\n",
    "test_vectors = np.vstack([sentence_vector(tokens, w2v_model) for tokens in test_tokens])\n",
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b56af2",
   "metadata": {},
   "source": [
    "## Поиск ближайших предложений (Word2Vec)\n",
    "Снова находим топ-3 похожих примера, но уже с помощью средних векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3929165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестовое предложение: Только бы не вылететь.\n",
      "Топ-3 похожих предложения:\n",
      "  score=0.998 | Они вылетели из Барроу.\n",
      "  score=0.995 | Шпилька вылетела\n",
      "  score=0.991 | Виктор так и не вылетел.\n",
      "\n",
      "Тестовое предложение: И как ты только справляешься, папа, таская эти коробки взад-вперед целый день.\n",
      "Топ-3 похожих предложения:\n",
      "  score=0.992 | Ольга, мне кажется, что будет неплохо, если ты поживешь пару дней у нас с Элизабет.\n",
      "  score=0.989 | С сегодняшнего дня для перепихона далеко ехать не придётся.\n",
      "  score=0.989 | Счастливого дня благодарения, доктор Кастеллано.\n",
      "\n",
      "Тестовое предложение: Возможно, у нас есть небольшое преимущество в переговорах.\n",
      "Топ-3 похожих предложения:\n",
      "  score=0.959 | Я решил предложить им одно из самых желанных лондонских владений.\n",
      "  score=0.954 | Чтобы найти оптимальный баланс, можно сделать серию снимков с различными значениями выдержки и экспозиции вспышки.\n",
      "  score=0.952 | www.mebeam.com - Здесь вы можете создать пространство и все, кто пришел в это пространство может быть видно и слышно. Не так комфортно операция, как в сторону RZ-Аахен, но некоторые неофициальные Adminiatrationsbefehle что вы пропустите на RZ.\n",
      "\n",
      "Тестовое предложение: Сколько времени вы будете делать то, что ему нужно?\n",
      "Топ-3 похожих предложения:\n",
      "  score=0.975 | Слушай, нужно сообщить время смерти Нины начальству, но я, сколько могу, потяну время.\n",
      "  score=0.973 | Мне нужно было время, чтобы подумать.\n",
      "  score=0.972 | Не думаю, что он сможет там сфотографироваться в ближайшее время.\n",
      "\n",
      "Тестовое предложение: 1 апреля Президент НКР Бако Саакян принял начальника Генштаба Вооруженных сил Республики Армения генерал-полковника Юрия Хачатурова.\n",
      "Топ-3 похожих предложения:\n",
      "  score=0.974 | Глава государства Александр Лукашенко назначил Николая Селиванова первым заместителем Управляющего делами Президента Республики Беларусь. Соответствующий Указ Президент Республики Беларусь подписал 20 февраля.\n",
      "  score=0.973 | 10 сентября Председатель НС Г.Саакян принял Чрезвычайного и Полномочного посла Республики Беларусь в Армении Игоря Назарука. Приветствуя новоназначенн...\n",
      "  score=0.969 | против президента Южно-Африканской Республики Конституционный суд Южной Африки постановил:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    vector = test_vectors[i].reshape(1, -1)\n",
    "    matches = top_k_similar(vector, train_vectors)\n",
    "    sentence = test_df.loc[i, 'ru']\n",
    "    print(f'Тестовое предложение: {sentence}')\n",
    "    print('Топ-3 похожих предложения:')\n",
    "    for idx, score in matches:\n",
    "        ref_sentence = train_df.loc[idx, 'ru']\n",
    "        print(f'  score={score:.3f} | {ref_sentence}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b295a06",
   "metadata": {},
   "source": [
    "## Вывод: Визуально TF-IDF справился лучше чем Word2Vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
